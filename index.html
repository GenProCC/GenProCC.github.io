<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Primary Meta Tags -->
    <title>GenProCC Workshop | NeurIPS 2025</title>
    <meta name="title" content="GenProCC Workshop | NeurIPS 2025">
    <meta name="description" content="Generative and Protective AI on Content Creation.">
    <meta name="keywords" content="keywords, here, placeholder">
    <meta name="author" content="GenProCC Workshop Organizers">

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="imgs/logo.png">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://genprocc.github.io/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://genprocc.github.io/">
    <meta property="og:title" content="GenProCC Workshop | NeurIPS 2025">
    <meta property="og:description" content="Generative and Protective AI on Content Creation.">
    <meta property="og:image" content="https://genprocc.github.io/imgs/logo.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://genprocc.github.io/">
    <meta name="twitter:title" content="GenProCC Workshop | NeurIPS 2025">
    <meta name="twitter:description" content="Generative and Protective AI on Content Creation.">
    <meta name="twitter:image" content="https://genprocc.github.io/imgs/logo.png">

    <!-- Schema.org for Google -->
    <!-- <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Event",
      "name": "GenProCC Workshop at NeurIPS 2025",
      "description": "Workshop description goes here.",
      "startDate": "YYYY-MM-DDTHH:MM:SSZ",
      "endDate": "YYYY-MM-DDTHH:MM:SSZ",
      "eventStatus": "https://schema.org/EventScheduled",
      "eventAttendanceMode": "https://schema.org/OfflineEventAttendanceMode",
      "location": {
        "@type": "Place",
        "name": "NeurIPS 2025 Venue",
        "address": {
          "@type": "PostalAddress",
          "addressLocality": "City",
          "addressRegion": "Region",
          "addressCountry": "Country"
        }
      },
      "organizer": {
        "@type": "Organization",
        "name": "GenProCC Workshop Organizing Committee",
        "url": "https://yourworkshop.github.io/"
      },
      "image": "https://yourworkshop.github.io/imgs/logo.png",
      "offers": {
        "@type": "Offer",
        "availability": "https://schema.org/InStock",
        "url": "https://yourworkshop.github.io/"
      }
    }
    </script> -->

    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">

    <script>
    // Placeholder deadlines
    // change the AoE time to UTC time
    // submission deadline: Aug 22, 2025 23:59 AoE
    // notification deadline: Sep 23, 2025 23:59 AoE
    // camera ready deadline: Oct 31, 2025 23:59 AoE
    // var submissionDeadline = new Date("Aug 23, 2025 11:59:00 UTC").getTime();
    var submissionDeadline = new Date("Aug 30, 2025 11:59:00 UTC").getTime();
    var notificationDeadline = new Date("Sep 24, 2025 11:59:00 UTC").getTime();
    var cameraReadyDeadline = new Date("Nov 1, 2025 11:59:00 UTC").getTime();

    function updateCountdown(deadline, elementId) {
        var now = new Date().getTime();
        var distance = deadline - now;
        var countdownElem = document.getElementById(elementId);
        if (distance < 0) {
            countdownElem.innerHTML = "00 weeks 00 days 00:00:00";
            countdownElem.style.color = "gray";
        } else {
            var totalDays = Math.floor(distance / (1000 * 60 * 60 * 24));
            var weeks = Math.floor(totalDays / 7);
            var days = totalDays % 7;
            var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
            var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
            var seconds = Math.floor((distance % (1000 * 60)) / 1000);
            var formatNumber = function(num) {
                return num < 10 ? "0" + num : num;
            };
            countdownElem.innerHTML = formatNumber(weeks) + " weeks " +
                formatNumber(days) + " days " + formatNumber(hours) + ":" +
                formatNumber(minutes) + ":" + formatNumber(seconds);
        }
        countdownElem.style.color = "black";
    }

    setInterval(function() {
        updateCountdown(submissionDeadline, "countdown_submission");
        updateCountdown(notificationDeadline, "countdown_notification");
        updateCountdown(cameraReadyDeadline, "countdown_camera");
    }, 1000);
    </script>
</head>

<body>
    <header class="fixed-header">
        <div class="container">
            <div class="logo">
                <div class="logo-container">
                    <img src="imgs/logo.png" alt="Workshop Logo" class="logo-image">
                    <div class="logo-text">
                        <h1>GenProCC</h1>
                        <p>NeurIPS 2025 Workshop</p>
                    </div>
                </div>
            </div>
            <nav>
                <ul>
                    <li><a href="#home" class="active">Home</a></li>
                    <li><a href="#about">About</a></li>
                    <li><a href="#call">Call for Papers</a></li>
                    <li><a href="#schedule">Schedule</a></li>
                    <li><a href="#speakers">Speakers</a></li>
                    <li><a href="#organizers">Organizers</a></li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </nav>
        </div>
    </header>

    <!-- Home/Hero Section -->
    <section id="home" class="hero">
        <div id="hero-background"></div>
        <div class="container">
            <h2 style="max-width: 1000px; background-color: rgba(0, 0, 0, 0.6);">Generative and Protective AI for Content Creation</h2>
            <p class="centered-subtitle" style="background-color: rgba(0, 0, 0, .6)">1st Workshop on GenProCC, NeurIPS 2025
                <br>
                Date: December 6, 2025 &nbsp;&middot;&nbsp; Room: Upper Level Room 23ABC &nbsp;&middot;&nbsp; Location: San Diego Convention Center, USA</p>
            <div class="cta-buttons">
                <a href="#call" class="btn primary">Submit Paper</a>
                <a href="#schedule" class="btn secondary" style="background-color: rgba(0, 0, 0, .5)">View Schedule</a>
                <a href="#posters" class="btn secondary" style="background-color: rgba(0, 0, 0, .5)">Accepted Papers</a>
            </div>
        </div>
    </section>

    <!-- About/Overview Section -->
    <section id="about" class="overview" style="margin-bottom: -60px;">
        <div class="container">
            <h2>Workshop Overview</h2>
            <div class="overview-content">
                <div class="overview-text">
                    <p style="text-align: justify;">
                        Recent advancements in generative AI (GenAI) have empowered machines to create high-quality content across diverse modalities - text, image, audio, and video - with impressive fluency and creativity. From GPT-4o and Stable Diffusion to Sora and MMAudio, the explosion of X-to-X generation (e.g., text-to-image, video-to-audio) is unlocking new frontiers in science, education, entertainment, and art.
                    </p>
                    <p style="text-align: justify;">
                        While GenAI has shown significant potential in creative applications (e.g., music, films, arts), these breakthroughs also raise pressing concerns related to safety, copyright, and ethical use. Generative models can be exploited to spread misinformation, violate intellectual property rights, or diminish human agency in creative processes. As such, there is an increasing need to balance <strong>innovation with protection</strong>, ensuring that AI-powered creative tools are used responsibly and ethically.
                    </p>
                    <p style="text-align: justify;"></p>
                        This workshop, GenProCC: Generative and Protective AI for Content Creation, brings together researchers, creators, and practitioners at the intersection of content generation and IP protection. By uniting the generative AI and creator communities, the GenProCC workshop aims to explore the latest advances, challenges, and opportunities in the rapidly evolving field.
                    </p>
                    <div class="topics" style="margin-top: 30px;">
                        <h3>Topics Include:</h3>
                        <ul>
                            <li><strong>Controllable Generative AI for Content Creation:</strong> This area advances generative models toward controllable X-to-X synthesis across any modalities (e.g., text, image, audio, video). Controllability is an emerging research frontier, enabling models to produce outputs that precisely follow user intent and empowering creators to integrate GenAI into sophisticated creative workflows.</li>
                            <li><strong>Protective AI Approaches for Content Creation:</strong> This area focuses on developing techniques to ensure the traceability and integrity of creator- or AI-generated content, including methods such as digital watermarking, fingerprinting, and provenance tracking. Moreover, benchmarking these protective techniques is essential for establishing robust standards and evaluating their real-world effectiveness. As generative models rapidly improve, safeguarding creators’ rights and mitigating broader societal risks are indispensable for the responsible AI deployment.</li>
                            <li><strong>Creative Practices with Generative AI:</strong> This area explores how artists and practitioners apply GenAI in real-world creative contexts, uncovering practical challenges, workflows, and ethical considerations. Insights from these studies help researchers align technical innovations with genuine user needs and inform the design of human-centered AI tools.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>


            <!-- Schedule Section -->
        <section id="schedule" class="schedule-section">
            <div class="container">
                <h2>Schedule</h2>
                <p><strong style="font-size: 1.2em; color: #2c3e50;">Workshop Schedule — December 6, 2025</strong></p>

                <div class="schedule-note">
                    <p><strong>Note:</strong> All times are local (San Diego).</p>
                </div>

                <div class="timeline">
                    <div class="timeline-item">
                        <div class="time">8:00 - 8:10</div>
                        <div class="event">
                            <h3>Opening Remarks</h3>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">8:10 - 8:55</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Mark Plumbley.png" alt="Mark Plumbley" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Mark Plumbley</h3>
                                    <p class="affiliation">Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">8:55 - 9:10</div>
                        <div class="event">
                            <h3>Guiding Audio Editing with Audio Language Model (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Zitong Lan, Yiduo Hao, Mingmin Zhao</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">9:10 - 9:45</div>
                        <div class="event">
                            <h3>Coffee break / Poster session</h3>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">9:45 - 10:30</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Luba Elliott.jpg" alt="Luba Elliott" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Luba Elliott</h3>
                                    <p class="affiliation">Creator Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">10:30 - 11:15</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Yu-Chiang Frank Wang.jpg" alt="Yu-Chiang Frank Wang" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Yu-Chiang Frank Wang</h3>
                                    <p class="affiliation">Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">11:15 - 11:30</div>
                        <div class="event">
                            <h3>EnTruth: Tracing the Unauthorized Dataset Usage in Diffusion Models (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Jie Ren, Yingqian Cui, Chen Chen, Yue XING, Hui Liu, Lingjuan Lyu</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">11:30 - 11:45</div>
                        <div class="event">
                            <h3>FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Yufan Zhou, Haoyu Shen, Huan Wang</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">11:45 - 12:30</div>
                        <div class="event">
                            <h3>Lunch break / Poster session</h3>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">12:30 - 13:15</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Chris Donahue.jpg" alt="Chris Donahue" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Chris Donahue</h3>
                                    <p class="affiliation">Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">13:15 - 14:00</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Adam Cole.jpg" alt="Adam Cole" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Adam Cole</h3>
                                    <p class="affiliation">Creator Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">14:00 - 14:45</div>
                        <div class="event">
                            <div class="event-with-photo">
                                <img src="imgs/speakers/Nanyun (Violet) Peng.png" alt="Nanyun (Violet) Peng" class="speaker-thumbnail">
                                <div class="event-details">
                                    <h3>Nanyun (Violet) Peng</h3>
                                    <p class="affiliation">Keynote</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">14:45 - 15:00</div>
                        <div class="event">
                            <h3>On Forging Semantic Watermarks in Diffusion Models: A Theoretical Perspective (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Cheng-Yi Lee, Yu-Feng Chen, Chun-Shien Lu, Jun-Cheng Chen</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">15:00 - 15:45</div>
                        <div class="event">
                            <h3>Coffee break + Poster session</h3>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">15:45 - 16:00</div>
                        <div class="event">
                            <h3>TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Kaustubh Ponkshe, Parjanya Prashant, Babak Salimi</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">16:00 - 16:15</div>
                        <div class="event">
                            <h3>Perturb your Data: Paraphrase-Guided Training Data Watermarking (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">16:15 - 16:30</div>
                        <div class="event">
                            <h3>Prefilled responses enhance zero-shot detection of AI-generated images (Oral)</h3>
                            <p class="authors"><strong>Authors:</strong> Zoher Kachwala, Danishjeet Singh, Danielle Yang, Filippo Menczer</p>
                        </div>
                    </div>

                    <div class="timeline-item">
                        <div class="time">16:30 - 17:00</div>
                        <div class="event">
                            <h3>Closing Remarks / Networking</h3>
                        </div>
                    </div>
                </div>
            </div>
        </section>

<!-- Poster Presentations / Accepted Papers -->
    <section id="posters" class="posters-section">
        <div class="container">
            <h2>Accepted Papers</h2>

            <div class="posters-content">
                <ol class="poster-list">
                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Multimodal Robustness Benchmark for Concept Erasure in Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Ju Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Localized-Attention-Guided Concept Erasure for Text-to-Image Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Zhuan Shi, Alireza Farashah, Rik de Vries, Golnoosh Farnadi</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip"</h4>
                        </div>
                        <p><strong>Authors:</strong> Wenqi Guo, Shan Du</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Bridging Reading Accessibility Gaps: Responsible Multimodal Simplification with Generative AI"</h4>
                        </div>
                        <p><strong>Authors:</strong> Sharv Murgai, Shivatmica Murgai</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Dynamic VLM-Guided Negative Prompting for Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Hoyeon Chang, Seungjin Kim, Yoonseok Choi</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"My Art My Choice: Adversarial Protection Against Image Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Ilke Demir, Anthony Rhodes, Ram Bhagat, Nese Alyuz, Sinem Aslan, Umur Ciftci</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Emotional Framing as a Control Channel: Effects of Prompt Valence on LLM Performance"</h4>
                        </div>
                        <p><strong>Authors:</strong> Enmanuel Felix-Pena, Tiki Li, Ayo Akinkugbe, Kevin Zhu, Shu Ze (Wayne) Chen, Ethan Hin</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Evolve to Inspire: Novelty Search for Diverse Image Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Passawis Chaiyapattanaporn, Alex Inch, Yuan Lu, Yuchen Zhu, Ting-Wen Ko, Davide Paglieri</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Membership and Dataset Inference Attacks on Large Audio Generative Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Jakub Proboszcz, Paweł Kochański, Karol Korszun, Katarzyna Stankiewicz, Giorgio Strano, Donato Crisostomi, Emanuele Rodolà, Kamil Deja, Jan Dubiński</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"SHIELD: A Benchmark Study on Zero-Shot Detection of AI-Edited Images with Vision Language Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Siyuan Cheng, Hanxi Guo, Zhenting Wang, Xiangyu Zhang, Lingjuan Lyu</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Explainable AI-Generated Image Detection RewardBench"</h4>
                        </div>
                        <p><strong>Authors:</strong> Michael Yang, Shijian Deng, William Doan, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Not All That’s Colorful Is Real: Rethinking Metrics for Image Colorization"</h4>
                        </div>
                        <p><strong>Authors:</strong> Swarnim Maheshwari, Syed Ali, Arkaprava Majumdar, Panshul Jindal, Vineeth N Balasubramanian</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"GenAIM: A Multimodal Artificial Intelligence Music Generation Web Tool Using Lyrics or Images"</h4>
                        </div>
                        <p><strong>Authors:</strong> Callie Liao, Ellie Zhang, Duoduo Liao</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"CPT: Controllable & Editable Design Variations with Language Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Karthik Suresh, Amine Ben Khalifa, Li Zhang, Wei-ting Hsu, Fangzheng Wu, Vinay More, Asim Kadav</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Seyed Amir Kasaei, Mohammad Hossein Rohban</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"SpotEdit: Evaluating Visually-Guided Image Editing Methods"</h4>
                        </div>
                        <p><strong>Authors:</strong> Sara Ghazanfari, Wei-An Lin, Haitong Tian, Ersin Yumer</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code"</h4>
                        </div>
                        <p><strong>Authors:</strong> Shriyansh Agrawal, Aidan Lau, Sanyam Shah, Ahan M R</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Jiwoo Shin, Byeonghu Na, Mina Kang, Wonhyeok Choi, Il-chul Moon</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"EgoAnimate: Generating Human Animations from Egocentric top-down Views via Controllable Latent Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Gurbuz Turkoglu, Julian Tanke, Iheb Belgacem, Lev Markhasin</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Accelerate Creation of Product Claims Using Generative AI"</h4>
                        </div>
                        <p><strong>Authors:</strong> PO-YU LIANG, Yong Zhang, Tatiana Hwa, Aaron Byers</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Video"</h4>
                        </div>
                        <p><strong>Authors:</strong> Alexander Htet Kyaw, Lenin Sivalingam</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge"</h4>
                        </div>
                        <p><strong>Authors:</strong> Fahad Shamshad, Tameem Bakr, Yahia Salaheldin Shaaban, Noor Hussein, Karthik Nandakumar, Nils Lukas</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech"</h4>
                        </div>
                        <p><strong>Authors:</strong> Jinju Kim, Taesoo Kim, Dong Kim, Jong Hwan Ko, Gyeong-Moon Park</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Decomate: Leveraging Generative Models for Co-Creative SVG Animation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Jihyeon Park, Jiyoon Myung, Seoni Shin, JUNGKI SON, Joohyung Han</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs"</h4>
                        </div>
                        <p><strong>Authors:</strong> Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Generative AI Agents for Controllable and Protected Content Creation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Haris Khan, Sadia Asif</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Comparing Occupational Gender Bias in AI-Generated Anime-style and Realistic Illustrations"</h4>
                        </div>
                        <p><strong>Authors:</strong> Ayano Ito, Shoma Mizuno, Keiichi Namikoshi, Yuko Sakurai, Satoshi Oyama</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Watermarking Diffusion Language Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Thibaud Gloaguen, Robin Staab, Nikola Jovanović, Martin Vechev</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Girish Koushik, Fatemeh Nazarieh, Katherine Birch, Shenbin Qian, Diptesh Kanojia</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"FlashFoley: Fast Interactive Sketch2Audio Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Zachary Novack, Koichi Saito, Zhi Zhong, Takashi Shibuya, Shuyang Cui, Julian McAuley, Taylor Berg-Kirkpatrick, christian simon, Shusuke Takahashi, Yuki Mitsufuji</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"LegalWiz: A Multi-Agent Generation Framework for Contradiction Detection in Legal Documents"</h4>
                        </div>
                        <p><strong>Authors:</strong> Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji, Nand Dave, Anudha Mittal</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Bridging_Reading_Accessibility_Gaps__Responsible_Multimodal_Simplification_with_Generative_AI"</h4>
                        </div>
                        <p><strong>Authors:</strong> Sharv Murgai, Shivatmica Murgai</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag Delul</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"MITHRIL: A Multi-modal and Immersive Generative AI Approach for Roleplaying Tabletop Games"</h4>
                        </div>
                        <p><strong>Authors:</strong> Michael Atkins, William Denman, Tuna Han Salih Meral, Mustafa Doga Dogan, Pinar Yanardag Delul</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Side Effects of Erasing Concepts from Diffusion Models"</h4>
                        </div>
                        <p><strong>Authors:</strong> Shaswati Saha, Sourajit Saha, Manas Gaur, Tejas Gokhale</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations"</h4>
                        </div>
                        <p><strong>Authors:</strong> Shaina Raza, Maximus Powers, PARTHA PRATIM SAHA, Mahveen Raza, Rizwan Qureshi</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Patch’n Play: Zero-Shot Video Editing by Fusing Local and Global Patches"</h4>
                        </div>
                        <p><strong>Authors:</strong> Hidir Yesiltepe, Yusuf Dalva, Ritika Allada, Pinar Yanardag Delul</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in Finance"</h4>
                        </div>
                        <p><strong>Authors:</strong> Sayash Raaj Hiraou</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Adaptive Originality Filtering: Rejection‑Based Prompting and RiddleScore for Culturally Grounded Multilingual Riddle Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Duy Le, Kent Ziti, Evan Girard-Sun, Vasu Sharma, Sean O'Brien, Kevin Zhu</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Carbon Literacy for Generative AI: Visualizing Training Emissions Through Human-Scale Equivalents"</h4>
                        </div>
                        <p><strong>Authors:</strong> Mahveen Raza, Maximus Powers</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Dynamic Guardrail Generation (DGG): A Framework for Prompt-Time Mitigation of LLM Harms"</h4>
                        </div>
                        <p><strong>Authors:</strong> Anh Dao Minh Nguyen</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Seyed Amir Kasaei, Ali Aghayari, Arash Mari Oriyad, Niki Sepasian, MohammadAmin Fazli, Mahdieh Soleymani, Mohammad Hossein Rohban</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Setting the DC: Tool-Grounded D&D Simulations to Test LLM Agents"</h4>
                        </div>
                        <p><strong>Authors:</strong> Shengqi Li, Ziyi Zeng, Jiajun Xi, Andrew Zhu, Prithviraj Ammanabrolu</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow"</h4>
                        </div>
                        <p><strong>Authors:</strong> Kenan Tang, Yanhong Li, Yao Qin</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Differentially Private Adaptation of Diffusion Models via Noisy Aggregated Embeddings"</h4>
                        </div>
                        <p><strong>Authors:</strong> Pura Peetathawatchai, Wei-Ning Chen, Berivan Isik, Sanmi Koyejo, Albert No</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Not All Deepfakes Are Created Equal: Triaging Audio Forgeries for Robust Deepfake Singer Identification"</h4>
                        </div>
                        <p><strong>Authors:</strong> Davide Salvi, Hendrik Vincent Koops, Elio Quinton</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation"</h4>
                        </div>
                        <p><strong>Authors:</strong> Ashwin Nagarajan, Hao-Wen Dong</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"Instance-Specific Test-Time Training for Speech Editing in the Wild"</h4>
                        </div>
                        <p><strong>Authors:</strong> Taewoo Kim, Uijong Lee, Hayoung Park, Choongsang Cho, Nam Park, Young Lee</p>
                    </li>

                    <li class="poster-item">
                        <div class="poster-title-row">
                            <h4>"ShrutiSense: Microtonal Modeling and Correction in Indian Classical Music"</h4>
                        </div>
                        <p><strong>Authors:</strong> Rajarshi Ghosh, Jayanth Athipatla</p>
                    </li>
                </ol>
            </div>
        </div>
    </section>

    <!-- Important Dates Section -->
    <section id="dates" class="dates-section">
        <div class="container">
            <h2>Important Dates</h2>
            <div class="dates-container">
                <div class="date-item">
                    <div class="date-icon">
                        <i class="fas fa-file-alt"></i>
                    </div>
                    <div class="date-info">
                        <h3>Paper Submission</h3>
                        <p><del>August 22, 2025 23:59 AoE</del></p>
                        <p><del>August 29, 2025 23:59 AoE</del></p>
                        <div id="countdown_submission"></div>
                    </div>
                </div>
                <div class="date-item">
                    <div class="date-icon">
                        <i class="fas fa-envelope-open-text"></i>
                    </div>
                    <div class="date-info">
                        <h3>Decision Notification</h3>
                        <p><del>September 23, 2025 AoE</del></p>
                        <div id="countdown_notification"></div>
                    </div>
                </div>
                <div class="date-item">
                    <div class="date-icon">
                        <i class="fas fa-check-circle"></i>
                    </div>
                    <div class="date-info">
                        <h3>Camera Ready</h3>
                        <p><del>October 31, 2025 23:59 AoE</del></p>
                        <div id="countdown_camera"></div>
                    </div>
                </div>
                <div class="date-item">
                    <div class="date-icon">
                        <i class="fas fa-calendar-day"></i>
                    </div>
                    <div class="date-info">
                        <h3>Workshop Date</h3>
                        <p>December 6, 2025 (Upper Level Room 23ABC)</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Call for Papers Section -->
    <section id="call" class="call-section">
        <div class="container">
            <h2>Call for Papers & Demos</h2>
            <div class="paper-info">
                <div class="submission-box">
                    <h4>Important Dates</h4>
                    <ul>
                        <li><del>
                            <strong style="color: #d32f2f;">Paper Submission Deadline:</strong>
                            <span style="color: #d32f2f;">August 22, 2025 23:59 AoE (Anywhere on Earth)</span>
                        </del></li>
                        <del><li>
                            <strong style="color: #d32f2f;">Paper Submission Deadline:</strong>
                            <span style="color: #d32f2f;">August 29, 2025 23:59 AoE (Anywhere on Earth)</span>
                        </li></del>
                        <del><li><strong>Decision Notification:</strong> September 23, 2025 AoE (Anywhere on Earth)</li></del>
                        <del><li><strong>Camera Ready Deadline:</strong> October 31, 2025 23:59 AoE (Anywhere on Earth)</li></del>
                        <li><strong>Workshop Date:</strong> December 6, 2025 </li>
                    </ul>
                </div>
                <p><strong>Submit the paper through <a href="https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/GenProCC" target="_blank"><i class="fas fa-external-link-alt"></i> OpenReview Portal</a></strong></p>
                <p>All accepted papers are expected to be presented in person during the poster session, and some of which will be selected for in-person oral talks at the workshop.</p>
                <br>
                <h3>Call for Regular/Short Papers</h3>
                <p>
                    We invite research contributions related to generative AI for content creation that is <strong> (1) original and unpublished; (2) work-in-progress; (3) published at recent conferences or journals (e.g., ICML, ICLR, NeurIPS, CVPR, ICCV, TMLR, etc.)</strong>, with an emphasis on controllability, protection, and real-world creative practices.
                    If the paper was already published at a conference or journal, please explicitly provide the link/conference name to the paper.
                    Topics of interest based on the workshop scope include, but not limited to:
                </p>
                <ul class="topics-list" style="margin-top: 20px;">
                    <li>Controllable X-to-X generation, where X represents any modality (e.g., text, image, audio, video)</li>
                    <li>Interactive or iterative generation pipelines on content creation (e.g., via codes, agents)</li>
                    <li>Evaluations and benchmarks for controllability</li>
                    <li>Applications of controllability/protection in creative practices</li>
                    <li>Digital watermarking, fingerprinting, and provenance tracking</li>
                    <li>Benchmarks for evaluating protection in generative systems</li>
                    <li>Case studies and design research for content creation</li>
                    <li>Human-in-the-loop approaches for real-world GenAI creation workflows</li>
                    <li>Emerging applications of GenAI in content creation</li>
                </ul>

                <h3>Call for Demos</h3>
                <p>
                    We welcome submissions that is <strong> (1) original and unpublished; (2) work-in-progress; (3) published at recent conferences or journals (e.g., ICML, ICLR, NeurIPS, CVPR, ICCV, TMLR, CHI, SIGGRAPH, etc.)</strong> from artists, designers, and interdisciplinary researchers from academic prototypes and early-stage creative tools exploring human-AI collaboration, novel interfaces, or protective techniques integrated into creative tools.
                    If the paper was already published at a conference or journal, please explicitly provide the link/conference name to the paper.
                    Topics of interest include, but not limited to:
                </p>
                <ul class="topics-list" style="margin-top: 20px;">
                    <li>AI-assisted (controllable) content creation (e.g., art, music, fashion, or performance)</li>
                    <li>New creative workflows or practices enabled by GenAI</li>
                    <li>Human-in-the-loop co-creation systems and user experience studies</li>
                    <li>Social, ethical, or cultural impacts of GenAI on creators and audiences</li>
                    <li>Evaluations of GenAI tools in creative education, prototyping, or curation</li>
                    <li>Interdisciplinary tools or frameworks bridging AI and creative practice</li>
                    <li>Emerging applications of GenAI in content creation</li>
                </p>
                <h3>Submission Guidelines</h3>
                <p>
                    The workshop accepts research, industrial, and creative demo papers of the following types:
                </p>
                <ul class="topics-list" style="margin-top: 20px;">
                    <li>Regular paper: 8 pages + references and appendix.</li>
                    <li>Short paper: 4 pages + references and appendix.</li>
                    <li>Creative demo: 2 pages + references and appendix.</li>
                </ul>
                <p>
                    Submissions should be anonymized and follow the <a href="https://media.neurips.cc/Conferences/NeurIPS2025/Styles.zip">NeurIPS 2025 LaTeX style</a>.
                    Please use <code>\usepackage[dblblindworkshop]{neurips_2025}</code> for submission and
                    <code>\usepackage[dblblindworkshop, final]{neurips_2025}</code> for the camera-ready version.
                </p>
                
                <br>
                <h3>Review Process</h3>
                <p>
                    All submissions will undergo a double-blind review process. Please ensure that your submission does not contain any identifying information about the authors.
                </p>
                <br>
                <h3>Publication</h3>
                <p>
                    Accepted papers will be made available through the OpenReview website and displayed on the GenProCC 2025 homepage, but are to be considered <strong>non-archival</strong>. Authors of accepted papers retain the full copyright of their work and are free to submit extended versions to conferences or journals.
                </p>
            </div>
        </div>
    </section>

    <!-- Call for Demos Section
    <section id="call" class="call-section-2">
        <div class="container">
            <h2>Call for Demo</h2>
            <p>
                We also welcome demo submissions. Details and submission instructions will be provided here.
            </p>
            <p>
                Submission Guidelines: TBA
            </p>
        </div>
    </section> -->

    <!-- Speakers Section -->
    <section id="speakers" class="speakers-section">
        <div class="container">
            <h2>Keynote Speakers</h2>
            <p>
                The invited speakers are organized into two complementary categories: researchers advancing GenAI (Tech) as well as creators and artists (Creators), listed below in alphabetical order by last name.
            </p>
            <h3 style="margin-top:30px;">Tech Speakers</h3>
            <div class="speakers-grid full">
                <!-- Tech speakers -->
                <div class="speaker-card">
                    <a href="https://chrisdonahue.com/" target="_blank">
                        <img src="imgs/speakers/Chris Donahue.jpg" alt="Chris Donahue">
                        <div class="speaker-info">
                            <h3>Chris Donahue</h3>
                            <p class="affiliation">Carnegie Mellon University & Google DeepMind</p>
                        </div>
                    </a>
                </div>
                <div class="speaker-card">
                    <a href="https://violetpeng.github.io/" target="_blank">
                        <img src="imgs/speakers/Nanyun (Violet) Peng.png" alt="Nanyun (Violet) Peng">
                        <div class="speaker-info">
                            <h3>Nanyun (Violet) Peng</h3>
                            <p class="affiliation">University of California, Los Angeles & Amazon</p>
                        </div>
                    </a>
                </div>
                <div class="speaker-card">
                    <a href="https://www.kcl.ac.uk/people/mark-plumbley" target="_blank">
                        <img src="imgs/speakers/Mark Plumbley.png" alt="Mark Plumbley">
                        <div class="speaker-info">
                            <h3>Mark Plumbley</h3>
                            <p class="affiliation">King’s College London</p>
                        </div>
                    </a>
                </div>
                <div class="speaker-card">
                    <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">
                        <img src="imgs/speakers/Yu-Chiang Frank Wang.jpg" alt="Yu-Chiang Frank Wang">
                        <div class="speaker-info">
                            <h3>Yu-Chiang Frank Wang</h3>
                            <p class="affiliation">National Taiwan University & Nvidia</p>
                        </div>
                    </a>
                </div>
            </div>
            <h3 style="margin-top:40px;">Creator Speakers</h3>
            <div class="speakers-grid full">
                <!-- Art speakers -->
                <div class="speaker-card">
                    <a href="https://www.adamcole.studio/about" target="_blank">
                        <img src="imgs/speakers/Adam Cole.jpg" alt="Adam Cole">
                        <div class="speaker-info">
                            <h3>Adam Cole</h3>
                            <p class="affiliation">University of the Arts London</p>
                        </div>
                    </a>
                </div>
                <div class="speaker-card">
                    <a href="https://elluba.com/" target="_blank">
                        <img src="imgs/speakers/Luba Elliott.jpg" alt="Luba Elliott">
                        <div class="speaker-info">
                            <h3>Luba Elliott</h3>
                            <!-- <p class="affiliation">TBD</p> -->
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- Organizers Section -->
    <section id="organizers" class="organizers-section">
        <div class="container">
            <h2>Organizers</h2>
            <div class="organizers-grid">
                <div class="organizer-card">
                    <a href="https://wywywang.github.io/" target="_blank">
                        <img src="imgs/organizers/Wei-Yao Wang.jpg" alt="Wei-Yao Wang">
                        <h3>Wei-Yao Wang</h3>
                        <p class="affiliation">Sony Group Corporation</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://scholar.google.com/citations?user=XCRO260AAAAJ" target="_blank">
                        <img src="imgs/organizers/takashi_shibuya.jpeg" alt="Takashi Shibuya">
                        <h3>Takashi Shibuya</h3>
                        <p class="affiliation">Sony AI</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://researchers.arts.ac.uk/1813-vali-lalioti" target="_blank">
                        <img src="imgs/organizers/Vali Lalioti.jpg" alt="Vali Lalioti">
                        <h3>Vali Lalioti</h3>
                        <p class="affiliation">University of the Arts London</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://qiyuw.github.io/" target="_blank">
                        <img src="imgs/organizers/Qiyu Wu.jpg" alt="Qiyu Wu">
                        <h3>Qiyu Wu</h3>
                        <p class="affiliation">Sony Group Corporation</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://web.cs.ucla.edu/~weiwang/" target="_blank">
                        <img src="imgs/organizers/Wei Wang.jpg" alt="Wei Wang">
                        <h3>Wei Wang</h3>
                        <p class="affiliation">University of California, Los Angeles</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://scholar.google.com/citations?user=_mhxayYAAAAJ" target="_blank">
                        <img src="imgs/organizers/Shusuke Takahashi.jpg" alt="Shusuke Takahashi">
                        <h3>Shusuke Takahashi</h3>
                        <p class="affiliation">Sony Group Corporation</p>
                    </a>
                </div>
                <div class="organizer-card">
                    <a href="https://www.yukimitsufuji.com/" target="_blank">
                        <img src="imgs/organizers/Yuki Mitsufuji.jpg" alt="Yuki Mitsufuji">
                        <h3>Yuki Mitsufuji</h3>
                        <p class="affiliation">Sony AI</p>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- Program Committee Section -->
    <section id="committee" class="committee-section">
        <div class="container">
            <h2>Program Committee</h2>
            <p>
                We gratefully acknowledge the following reviewers for their contributions to the GenProCC Workshop program committee:
            </p>
            <ul class="committee-list" style="columns: 2; max-width: 700px; margin: 30px auto 0 auto; font-size: 1.1em;">
                <li>Adam Cole</li>
                <li>Akio Hayakawa</li>
                <li>Alexander K Taylor</li>
                <li>An-Zi Yen</li>
                <li>Bodhisatta Maiti</li>
                <li>christian simon</li>
                <li>Dawei Li</li>
                <li>Dhivya Nagasubramanian</li>
                <li>Dongseok Shim</li>
                <li>Fahad Shamshad</li>
                <li>Fu-En Yang</li>
                <li>Julian Tanke</li>
                <li>Kazuki Shimada</li>
                <li>Kengo Uchida</li>
                <li>Kim Jun-Seong</li>
                <li>Koichi Saito</li>
                <li>Kuang-Da Wang</li>
                <li>Megha Sharma</li>
                <li>Mingfei Guo</li>
                <li>Minghao Xie</li>
                <li>Rohan Wadhawan</li>
                <li>Ryo Hachiuma</li>
                <li>Ryokan Ri</li>
                <li>Wei-Wei Du</li>
                <li>Xuesong Yang</li>
                <li>Yu Zhou</li>
                <li>Yu-Chien Tang</li>
                <li>Yuya Kobayashi</li>
                <li>Zheng WEI</li>
                <li>Zhongtao Miao</li>
            </ul>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="contact-section">
        <div class="container">
            <h2>Contact Us</h2>
            <div class="contact-info">
                <div class="contact-method">
                    <div class="icon">
                        <i class="fas fa-envelope"></i>
                    </div>
                    <div class="details">
                        <h3>Email</h3>
                        <p>For general inquiries:</p>
                        <p><a href="mailto:Wei-yao.Wang@sony.com">Wei-yao.Wang@sony.com</a></p>
                        <p><a href="mailto:Takashi.Tak.Shibuya@sony.com">Takashi.Tak.Shibuya@sony.com</a></p>
                    </div>
                </div>
                <div class="contact-method">
                    <div class="icon">
                        <i class="fas fa-map-marker-alt"></i>
                    </div>
                    <div class="details">
                        <h3>Workshop Location</h3>
                        <p>NeurIPS 2025</p>
                        <p>San Diego, USA</p>
                        <p>San Diego Convention Center</p>
                        <p>Upper Level Room 23ABC</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <h3>GenProCC Workshop</h3>
                    <p>NeurIPS 2025 | San Diego, USA</p>
                </div>
                <div class="footer-links">
                    <ul>
                        <li><a href="#home">Home</a></li>
                        <li><a href="#about">About</a></li>
                        <li><a href="#call">Call for Papers</a></li>
                        <li><a href="#schedule">Schedule</a></li>
                        <li><a href="#speakers">Speakers</a></li>
                        <li><a href="#organizers">Organizers</a></li>
                        <li><a href="#contact">Contact</a></li>
                    </ul>
                </div>
                <div class="footer-contact">
                    <p><strong>Contact:</strong></p>
                    <p>Wei-Yao Wang (<a href="mailto:Wei-yao.Wang@sony.com">Wei-yao.Wang@sony.com</a>)</p>
                    <p>Takashi Takahashi (<a href="mailto:Takashi.Tak.Shibuya@sony.com">Takashi.Tak.Shibuya@sony.com</a>)</p>
                </div>
            </div>
            <div class="copyright">
                <p>&copy;2025 GenProCC Workshop. All rights reserved. We thanks the organizers of <a href="https://3d-llm-vla.github.io/">3D-LLM/VLA workshop</a> for the website template.</p>
            </div>
        </div>
    </footer>

    <script src="js/script.js"></script>

    <div style="display:none;">
        <script type="text/javascript" id="clustrmaps"
            src="//clustrmaps.com/map_v2.js?d=placeholder"></script>
    </div>
</body>

</html>
